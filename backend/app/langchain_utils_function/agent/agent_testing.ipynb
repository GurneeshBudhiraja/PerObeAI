{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from langchain_openai import ChatOpenAI ## for openAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI ## for gemini\n",
    "from langchain_google_vertexai import VertexAI, ChatVertexAI, VertexAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.tools import tool\n",
    "from pinecone.grpc import PineconeGRPC as pinecone_grpc\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import vertexai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # load environment variables\n",
    "vertexai.init(project=os.getenv(\"PROJECT_ID\"), location=os.getenv(\"PROJECT_LOCATION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatVertexAI(model=\"gemini-1.5-pro-001\") ## llm for the agent\n",
    "llm= ChatOpenAI(model=\"gpt-4o\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm ChatGPT, created by OpenAI.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is your name and which company made you in no more than 10 words\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeClass:\n",
    "  try:\n",
    "\n",
    "    def __init__(self,user_id:str) -> None:\n",
    "      self.user_id = user_id\n",
    "      self.pc = pinecone_grpc(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "      ## pinecone db index\n",
    "      self.index = self.pc.Index(os.getenv(\"PINECONE_INDEX_NAME\"))\n",
    "    \n",
    "    def insert_vectors(self,images_vector:list[dict])->int:\n",
    "      upsert_response = self.index.upsert(\n",
    "        vectors=images_vector,\n",
    "        namespace=self.user_id,\n",
    "      )\n",
    "      return upsert_response.upserted_count ## returns the total number of vectors inserted/updated\n",
    "\n",
    "    def fetch_similar_vectors(self, vector_list:list[float|int], top_k:int=3,include_values:bool=False,filter:dict={},include_metadata:bool=False):\n",
    "      similar_vectors = self.index.query(\n",
    "        namespace=self.user_id,\n",
    "        vector=vector_list,\n",
    "        filter=filter,\n",
    "        top_k=top_k,\n",
    "        include_values=include_values,\n",
    "        include_metadata=include_metadata\n",
    "      )\n",
    "      return similar_vectors\n",
    "    \n",
    "  except Exception as e:\n",
    "    raise Exception(f\"Error in PineconeClass: {str(e)}\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RecommendedOutfit(BaseModel):\n",
    "  upperwear_url:str = Field( title=\"upperwear_url\",description=\"the url from the upperwear collection if the user asked for upperwear recommendation which when paired with the lowerwear collection will give the best outfit. Returns 'none' if the user did not ask for upperwear recommendation\")\n",
    "  lowerwear_url:str = Field(title=\"lowerwear_url\",description=\"the url from the lowerwear collection if the user asked for lowerwear recommendation which when paired with the upperwear collection will give the best outfit. Returns 'none' if the user did not ask for lowerwear recommendation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for retrieving the clothes of upperwear tag\n",
    "@tool\n",
    "def retrieve_upperwear(user_prompt_vector:list[float])->list[dict]:\n",
    "  \"\"\"fetches the top 3 upperwear vectors closest to the vector of the user_prompt_vector from the pinecone db vector store\"\"\"\n",
    "  print(\"GETTING THE UPPERWEAR COLLECTION\")\n",
    "  pinecone_class_instance = PineconeClass(user_id=\"JKVDl1ErPjaj3TPRNuBUsN3W9xS2\")\n",
    "  \n",
    "  upperwear_clothes_data = pinecone_class_instance.fetch_similar_vectors(vector_list=user_prompt_vector, top_k=4, filter={\"tag\":\"upperwear\"},include_metadata=True)[\"matches\"] ## list of dicts\n",
    "  upperwear_collection=[{\"type\":\"image_url\",\"image_url\":data[\"metadata\"][\"url\"]} for data in upperwear_clothes_data]\n",
    "  return upperwear_collection\n",
    "\n",
    "# for retrieving the clothes of lowerwear tag\n",
    "@tool\n",
    "def retrieve_lowerwear(user_prompt_vector:list[float])->list[dict]:\n",
    "  \"\"\"fetches the 3 lowerwear vectors closest to the vector of the user_prompt_vector from the pinecone db vector store\"\"\"\n",
    "  print(\"GETTING THE LOWERWEAR COLLECTION\")\n",
    "  pinecone_class_instance = PineconeClass(user_id=\"JKVDl1ErPjaj3TPRNuBUsN3W9xS2\")\n",
    "  lowerwear_collection=[]\n",
    "  lowerwear_clothes_data = pinecone_class_instance.fetch_similar_vectors(vector_list=user_prompt_vector, top_k=4, filter={\"tag\":\"lowerwear\"},include_metadata=True)[\"matches\"] ## list of dicts\n",
    "  \n",
    "  lowerwear_collection=[{\"type\":\"image_url\",\"image_url\":data[\"metadata\"][\"url\"]} for data in lowerwear_clothes_data]\n",
    "  return lowerwear_collection\n",
    "\n",
    "@tool\n",
    "def get_recommendation(user_prompt:str, upperwear_data,lowerwear_data)->dict:\n",
    "  \"\"\"will get the data of the upperwear and lowerwear after the data has been fetched from pinecone vector store db and generate the recommendation based on that data using the user_prompt\"\"\"\n",
    "  print(\"GENERATING  THE RECOMMENDATION\")\n",
    "  model = VertexAI(model_name=\"gemini-1.5-flash\",temperture=2, top_p=0.95) ## Model for generating the output\n",
    "\n",
    "  parser = JsonOutputParser(pydantic_object=RecommendedOutfit) ## Parser for parsing the output\n",
    "  \n",
    "  prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful bot that helps users answer questions based on the user input. You have to respond according to the {format_instructions}. In addition to the instructions, if the accessibility_type is blind then you have to provide the output explaining each image like how it feels so that the blind person can identify the cloth by feeling it. If the accessibility_type is some kind of color blind disorder then you have to explain the cloth in the color same as the color blind person can see.\"),\n",
    "    (\"human\", \"{user_prompt}. The uppperwear data is {upperwear_data} and the lowerwear data is {lowerwear_data}.\")\n",
    "  ])\n",
    "\n",
    "  chain = prompt | model | parser\n",
    "  recommendation = chain.invoke({\n",
    "    \"format_instructions\":parser.get_format_instructions(),\n",
    "    \"upperwear_data\":upperwear_data,\n",
    "    \"lowerwear_data\":lowerwear_data,\n",
    "    \"user_prompt\":user_prompt\n",
    "  })\n",
    "  return recommendation\n",
    "\n",
    "@tool\n",
    "def get_text_vector(user_prompt:str)->list[float]:\n",
    "  \"\"\"Will accept the user input and will return the vector of the user input\"\"\"\n",
    "  print(\"GENERATING THE EMBEDDINGS\")\n",
    "  embedding_model = VertexAIEmbeddings(model_name=\"multimodalembedding\")\n",
    "  user_prompt_vector = embedding_model.embed_query(text=user_prompt)\n",
    "  print(\"USER PROMPT VECTOR\",user_prompt_vector)\n",
    "  return user_prompt_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\", \"\"\"\n",
    "      You are a powerful AI bot whose job is to recommend the outfits to the user. You have to understand the user_prompt and then use the  tools provided to get the result. If the user_prompt involves a description of a clothing item that belongs to the category of upperwear you will retrieve the results for lowerwear. If the user_prompt involves the description of a clothing item that belongs to the lowerwear category you will fetch the results from the upperwear category. If the user_prompt does not mention any of the above you will get the results from both the categories\n",
    "      \"\"\"),\n",
    "      (\"human\", \"{user_prompt}.\"),\n",
    "      (\"placeholder\",\"{agent_scratchpad}\")\n",
    "    ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retrieve_upperwear,retrieve_lowerwear,get_recommendation,get_text_vector]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(prompt=prompt,llm=llm,tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression expected after dictionary key and ':' (581349981.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"format_instructions\":\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expression expected after dictionary key and ':'\n"
     ]
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "  \"user_prompt\":\"what should I wear for the beach party today evening in Toronto?\",\n",
    "  \"format_instructions\":\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatVertexAI(model_name=\"gemini-1.5-pro-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722133802.409219  985370 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am Bard, a large language model created by Google AI. I support function calling. \\n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is your name and which company made you in no more than 10 words and do you support function calling\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
